data:
  dataset_path: "data/raw/CEAS_08.csv"

  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

  batch_size: 32

  # Only used by RNN/TextVectorization; harmless for FNN
  max_tokens: 2000



model:
  type: "fnn"

  # Google NNLM en-128 embedding dimension
  embedding_dim: 128

  hidden_dim: 16

  # Number of dense+dropout blocks
  num_layers: 2

  dropout_rate: 0.2



train:
  epochs: 5

  optimizer: "adamw"

  # Same LR you used for BERT fine-tuning is fine for NNLM+FNN too
  learning_rate: 0.001

  # Make runs reproducible
  seed: 42

# -------------------------
# OUTPUT
# -------------------------
output_dir: "results/feedforward_double_dr02/"
